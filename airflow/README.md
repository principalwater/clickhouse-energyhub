# Airflow DAGs для Energy Hub

Этот каталог содержит DAG'и для Apache Airflow, которые управляют генерацией и обработкой данных в Energy Hub.

## Структура

```
airflow/
├── dags/                          # DAG файлы
│   ├── data_generation_pipeline.py    # Генерация данных речного стока (каждую минуту)
│   ├── market_data_pipeline.py        # Генерация рыночных данных (каждые 5 минут)
│   ├── static_data_pipeline.py        # Генерация статических данных (раз в день)
│   ├── data_processing_pipeline.py    # Обработка данных через слои (каждые 15 минут)
│   ├── clickhouse_backup_pipeline.py  # Бэкап и восстановление ClickHouse (каждый день)
│   ├── dbt_pipeline.py                # Пайплайн для работы с dbt (каждые 30 минут)
│   └── energy_data_pipeline.py        # Основной пайплайн (каждый час)
└── README.md                      # Этот файл
```

## DAG'и

### 1. data_generation_pipeline.py
**Расписание:** Каждую минуту (`* * * * *`)
**Описание:** Генерирует данные о речном стоке и отправляет их в Kafka топик `energy_data_1min`

### 2. market_data_pipeline.py
**Расписание:** Каждые 5 минут (`*/5 * * * *`)
**Описание:** Генерирует рыночные данные и отправляет их в Kafka топик `energy_data_5min`

### 3. static_data_pipeline.py
**Расписание:** Каждый день в 2:00 (`0 2 * * *`)
**Описание:** Генерирует статические справочные данные:
- Устройства (ГЭС, ТЭС, СЭС)
- Локации (регионы)
- Потребление энергии
- Погодные данные

### 4. data_processing_pipeline.py
**Расписание:** Каждые 15 минут (`*/15 * * * *`)
**Описание:** Обрабатывает данные через слои:
- Проверка качества данных (DQ)
- Обработка слоя ODS
- Обработка слоя DDS
- Обработка слоя CDM
- Запуск тестов
- Генерация документации

### 5. clickhouse_backup_pipeline.py
**Расписание:** Каждый день в 3:00 (`0 3 * * *`)
**Описание:** Бэкап и восстановление ClickHouse через clickhouse-backup

### 6. dbt_pipeline.py
**Расписание:** Каждые 30 минут (`*/30 * * * *`)
**Описание:** Пайплайн для работы с dbt моделями

### 7. energy_data_pipeline.py
**Расписание:** Каждый час (`0 * * * *`)
**Описание:** Основной пайплайн для обработки энергетических данных

## Интеграция

DAG'и интегрированы со следующими сервисами:
- **Kafka:** Для потоковой передачи данных
- **ClickHouse:** Для хранения и обработки данных
- **dbt:** Для трансформации данных
- **Nord Pool API:** Для получения рыночных данных (опционально)

## Зависимости

DAG'и используют скрипты из каталога `../scripts/`:
- `clickhouse_utils.py` - утилиты для работы с ClickHouse
- `clickhouse_backup_manager.py` - менеджер бэкапов ClickHouse
- `generate_static_data.py` - генерация статических данных
- `nordpool_api_client.py` - клиент для Nord Pool API
- `kafka_producer/` - генераторы данных для Kafka

## Развертывание

DAG'и автоматически копируются в контейнер Airflow при развертывании через Terraform. Путь к DAG'ам настраивается в переменной `airflow_dags_path`.

## Мониторинг

- **Airflow UI:** http://localhost:8080
- **Логи:** Доступны в веб-интерфейсе Airflow
- **Метрики:** Можно настроить через Prometheus/Grafana
