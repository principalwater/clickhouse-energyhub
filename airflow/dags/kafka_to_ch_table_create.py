from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
import json
import os
import sys
from dotenv import load_dotenv

# Load environment variables
dotenv_path = os.path.join(os.path.dirname(__file__), '../../infra/env/clickhouse.env')
load_dotenv(dotenv_path=dotenv_path)

def get_clickhouse_config():
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ"""
    host = os.getenv("CLICKHOUSE_HOST")
    port = os.getenv("CLICKHOUSE_PORT")
    username = os.getenv("CH_USER")
    password = os.getenv("CH_PASSWORD")
    
    missing_vars = []
    if not host:
        missing_vars.append("CLICKHOUSE_HOST")
    if not port:
        missing_vars.append("CLICKHOUSE_PORT")
    if not username:
        missing_vars.append("CH_USER")
    if not password:
        missing_vars.append("CH_PASSWORD")
    
    if missing_vars:
        raise ValueError(f"ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ClickHouse: {', '.join(missing_vars)}. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ñ„Ğ°Ğ¹Ğ» infra/env/clickhouse.env")
    
    return {
        'host': host,
        'port': int(port),
        'username': username,
        'password': password,
        'secure': False
    }

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² DAG
default_args = {
    'owner': 'energy-hub',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DAG Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸Ğ· Kafka Ğ² ClickHouse
kafka_to_ch_dag = DAG(
    'kafka_to_ch_table_create',
    default_args=default_args,
    description='Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† ClickHouse Ğ¸Ğ· Kafka Ñ‚Ğ¾Ğ¿Ğ¸ĞºĞ¾Ğ² (Kafka â†’ Materialized View â†’ Distributed)',
    schedule=None,  # Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº
    catchup=False,
    tags=['clickhouse', 'kafka', 'tables', 'kafka-to-clickhouse'],
)

def get_table_config(**context):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² DAG"""
    try:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° DAG
        dag_run = context['dag_run']
        conf = dag_run.conf if dag_run else {}
        
        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Kafka â†’ Materialized View â†’ Distributed Ğ² DWH Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ
        default_config = {
            'kafka_topic': 'energy_data_1min',
            'target_table_name': 'river_flow',
            'dwh_layer': 'raw',  # raw, ods, dds, cdm
            'sort_key': 'timestamp',
            'partition_key': 'toYYYYMM(timestamp)',
            'shard_key': 'xxHash64(timestamp)',
            'cluster_name': 'dwh_prod',
            'kafka_broker': 'kafka:9092',
            'schema': {
                'timestamp': 'String',
                'river_name': 'String',
                'ges_name': 'String',
                'water_level_m': 'Float64',
                'flow_rate_m3_s': 'Float64',
                'power_output_mw': 'Float64'
            }
        }
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
        config = {**default_config, **conf}
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ñ DWH
        dwh_layer = config['dwh_layer']
        if dwh_layer == 'raw':
            database_name = 'raw'
        elif dwh_layer == 'ods':
            database_name = 'ods'
        elif dwh_layer == 'dds':
            database_name = 'dds'
        elif dwh_layer == 'cdm':
            database_name = 'cdm'
        else:
            database_name = 'raw'  # Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
        
        config['database_name'] = database_name
        
        print(f"ğŸ“‹ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ Kafka â†’ Materialized View â†’ Distributed:")
        print(f"   Kafka Ñ‚Ğ¾Ğ¿Ğ¸Ğº: {config['kafka_topic']}")
        print(f"   Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°: {config['target_table_name']}")
        print(f"   Ğ¡Ğ»Ğ¾Ğ¹ DWH: {config['dwh_layer']}")
        print(f"   Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {config['database_name']}")
        print(f"   ĞšĞ»ÑÑ‡ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸: {config['sort_key']}")
        print(f"   ĞšĞ»ÑÑ‡ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {config['partition_key']}")
        print(f"   ĞšĞ»ÑÑ‡ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {config['shard_key']}")
        print(f"   ĞšĞ»Ğ°ÑÑ‚ĞµÑ€: {config['cluster_name']}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ² XCom Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…
        context['task_instance'].xcom_push(key='table_config', value=config)
        
        return config
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸: {e}")
        raise

def check_connections(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğº ClickHouse Ğ¸ Kafka"""
    try:
        import clickhouse_connect
        from kafka import KafkaConsumer
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        print("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ClickHouse Ñ‡ĞµÑ€ĞµĞ· HTTP Ğ¿Ğ¾Ñ€Ñ‚
        client = clickhouse_connect.get_client(**ch_config)
        
        result = client.query('SELECT version()')
        version = result.result_rows[0][0]
        print(f"âœ… ClickHouse: Ğ²ĞµÑ€ÑĞ¸Ñ {version}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        cluster_result = client.query(f"SELECT name, host_name, port FROM system.clusters WHERE name = '{config['cluster_name']}'")
        if cluster_result.result_rows:
            print(f"âœ… ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ {config['cluster_name']}: {len(cluster_result.result_rows)} ÑƒĞ·Ğ»Ğ¾Ğ²")
        else:
            print(f"âš ï¸ ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ {config['cluster_name']} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        
        client.close()
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Kafka
        consumer = KafkaConsumer(
            bootstrap_servers=[config['kafka_broker']],
            consumer_timeout_ms=5000
        )
        
        topics = consumer.topics()
        if config['kafka_topic'] in topics:
            print(f"âœ… Kafka Ñ‚Ğ¾Ğ¿Ğ¸Ğº {config['kafka_topic']} Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        else:
            print(f"âš ï¸ Kafka Ñ‚Ğ¾Ğ¿Ğ¸Ğº {config['kafka_topic']} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        
        consumer.close()
        
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹: {e}")
        raise

def generate_sql_script(**context):
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸"""
    try:
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        print("ğŸ”„ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°...")
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Kafka â†’ Materialized View â†’ Distributed
        kafka_topic = config['kafka_topic']
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        dwh_layer = config['dwh_layer']
        sort_key = config['sort_key']
        partition_key = config['partition_key']
        shard_key = config['shard_key']
        cluster_name = config['cluster_name']
        kafka_broker = config['kafka_broker']
        schema = config['schema']
        
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ Kafka â†’ Materialized View â†’ Distributed Ğ² DWH Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ
        sql_script = f"""
-- =====================================================
-- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ {target_table_name}
-- Ğ¡Ñ…ĞµĞ¼Ğ°: Kafka Topic â†’ Kafka Table Engine â†’ Materialized View â†’ ReplicatedMergeTree/Distributed
-- Ğ¡Ğ»Ğ¾Ğ¹ DWH: {dwh_layer}
-- Ğ¢Ğ¾Ğ¿Ğ¸Ğº: {kafka_topic}
-- =====================================================

-- Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ DWH Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
CREATE DATABASE IF NOT EXISTS energy_kafka ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS raw ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS ods ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS dds ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS cdm ON CLUSTER {cluster_name};

-- 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼ Kafka Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚Ğ¾Ğ¿Ğ¸ĞºĞ°
CREATE TABLE energy_kafka.{target_table_name}_kafka ON CLUSTER {cluster_name} (
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹
        for field_name, field_type in schema.items():
            sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = Kafka
SETTINGS
    kafka_broker_list = '{kafka_broker}',
    kafka_topic_list = '{kafka_topic}',
    kafka_group_name = 'clickhouse-{target_table_name}-consumer',
    kafka_format = 'JSONEachRow',
    kafka_num_consumers = 1,
    kafka_skip_broken_messages = 1000,
    kafka_row_delimiter = '\\n';

-- 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° MergeTree (ReplicatedMergeTree)
CREATE TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} (
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ timestamp
        for field_name, field_type in schema.items():
            if field_name == 'timestamp':
                sql_script += f"    {field_name} DateTime,\n"
            else:
                sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{{shard}}/{database_name}/{target_table_name}_local/{{uuid}}/', '{{replica}}')
PARTITION BY {partition_key}
ORDER BY ({sort_key})
PRIMARY KEY ({sort_key});

-- Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°
CREATE TABLE {database_name}.{target_table_name} ON CLUSTER {cluster_name} (
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ timestamp
        for field_name, field_type in schema.items():
            if field_name == 'timestamp':
                sql_script += f"    {field_name} DateTime,\n"
            else:
                sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = Distributed('{cluster_name}', '{database_name}', '{target_table_name}_local', {shard_key});

-- Materialized View
CREATE MATERIALIZED VIEW {database_name}.{target_table_name}_mv ON CLUSTER {cluster_name} 
TO {database_name}.{target_table_name} AS
SELECT
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ timestamp
        for field_name, field_type in schema.items():
            if field_name == 'timestamp':
                sql_script += f"    parseDateTimeBestEffort(timestamp) AS timestamp,\n"
            else:
                sql_script += f"    {field_name},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
FROM energy_kafka.{target_table_name}_kafka;

-- Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
ADD INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 4;

-- ĞŸÑ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
ADD PROJECTION {target_table_name}_projection (
    SELECT * ORDER BY {sort_key}
);

-- ĞœĞ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
MATERIALIZE PROJECTION {target_table_name}_projection;
"""
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ² XCom
        context['task_instance'].xcom_push(key='sql_script', value=sql_script)
        
        print(f"âœ… SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ {target_table_name}")
        print(f"ğŸ“‹ Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {len(sql_script)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
        print(f"ğŸ“‹ ĞŸĞµÑ€Ğ²Ñ‹Ğµ 500 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²: {sql_script[:500]}")
        
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {e}")
        raise

def execute_sql_script(**context):
    """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°"""
    try:
        import clickhouse_connect
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        sql_script = context['task_instance'].xcom_pull(task_ids='generate_sql_script', key='sql_script')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        if not sql_script:
            raise ValueError("SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° generate_sql_script Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        print("ğŸ”„ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ (Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ)
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
        lines = []
        for line in sql_script.split('\n'):
            line = line.strip()
            if line and not line.startswith('--'):
                lines.append(line)
        
        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
        queries = []
        current_query = []
        in_multiline = False
        
        for line in lines:
            if line.startswith('CREATE') or line.startswith('ALTER') or line.startswith('DROP'):
                # ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                if current_query:
                    queries.append(' '.join(current_query))
                current_query = [line]
                in_multiline = True
            elif in_multiline:
                current_query.append(line)
                if line.endswith(';'):
                    # Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                    queries.append(' '.join(current_query))
                    current_query = []
                    in_multiline = False
            else:
                # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                if line.endswith(';'):
                    queries.append(line)
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ
        if current_query:
            queries.append(' '.join(current_query))
        
        print(f"ğŸ“‹ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ {len(queries)} SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²...")
        print(f"ğŸ“‹ Ğ Ğ°Ğ·Ğ¼ĞµÑ€ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {len(sql_script)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
        print(f"ğŸ“‹ ĞŸĞµÑ€Ğ²Ñ‹Ğµ 500 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² SQL: {sql_script[:500]}")
        
        # ĞÑ‚Ğ»Ğ°Ğ´Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…
        for i, query in enumerate(queries, 1):
            print(f"ğŸ“‹ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}: {query[:100]}...")
        
        for i, query in enumerate(queries, 1):
            if query.strip():
                try:
                    print(f"ğŸ”„ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}/{len(queries)}: {query[:100]}...")
                    client.command(query)
                    print(f"âœ… Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i} Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½")
                except Exception as e:
                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ {i}: {e}")
                    print(f"âš ï¸ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ: {query}")
                    # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ
                    continue
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            kafka_tables = client.query(f"SHOW TABLES FROM energy_kafka LIKE '{target_table_name}_kafka'")
            if kafka_tables.result_rows:
                print(f"âœ… Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° energy_kafka.{target_table_name}_kafka ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âš ï¸ Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° energy_kafka.{target_table_name}_kafka Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            local_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}_local'")
            if local_tables.result_rows:
                print(f"âœ… Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name}_local ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âš ï¸ Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name}_local Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            dist_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}'")
            if dist_tables.result_rows:
                print(f"âœ… Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name} ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âš ï¸ Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Materialized View
        try:
            mv_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}_mv'")
            if mv_tables.result_rows:
                print(f"âœ… Materialized View {database_name}.{target_table_name}_mv ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âš ï¸ Materialized View {database_name}.{target_table_name}_mv Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Materialized View: {e}")
        
        client.close()
        
        print("âœ… SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾")
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {e}")
        raise

def verify_data_flow(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    try:
        import clickhouse_connect
        import time
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        
        print("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
        
        # Ğ–Ğ´ĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        print("â³ ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (30 ÑĞµĞºÑƒĞ½Ğ´)...")
        time.sleep(30)
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ
        try:
            count_result = client.query(f'SELECT count() FROM {database_name}.{target_table_name}')
            count = count_result.result_rows[0][0]
            print(f"ğŸ“Š Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name}: {count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
            
            if count > 0:
                # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                sample_result = client.query(f'SELECT * FROM {database_name}.{target_table_name} ORDER BY timestamp DESC LIMIT 1')
                if sample_result.result_rows:
                    print(f"ğŸ“‹ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· {database_name}.{target_table_name}:")
                    print(f"   {sample_result.result_rows[0]}")
            else:
                print(f"âš ï¸ Ğ’ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ {database_name}.{target_table_name} Ğ¿Ğ¾ĞºĞ° Ğ½ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
                
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        
        client.close()
        
        print("âœ… ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise

def health_check(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†"""
    try:
        import clickhouse_connect
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        
        print("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
        tables_status = client.query(f'''
            SELECT 
                database,
                table,
                engine,
                total_rows,
                total_bytes
            FROM system.tables 
            WHERE database IN ('{database_name}', 'energy_kafka')
            AND table LIKE '{target_table_name}%'
            ORDER BY database, table
        ''')
        
        print("ğŸ“‹ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†:")
        for row in tables_status.result_rows:
            print(f"   {row[0]}.{row[1]} ({row[2]}): {row[3]} ÑÑ‚Ñ€Ğ¾Ğº, {row[4]} Ğ±Ğ°Ğ¹Ñ‚")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Materialized Views
        mv_status = client.query(f'''
            SELECT 
                database,
                table,
                engine,
                engine_full
            FROM system.tables 
            WHERE engine = 'MaterializedView' 
            AND database IN ('{database_name}', 'energy_kafka')
            AND table LIKE '{target_table_name}%'
        ''')
        
        print("ğŸ“‹ Materialized Views:")
        for row in mv_status.result_rows:
            print(f"   {row[0]}.{row[1]} ({row[2]})")
        
        client.close()
        
        print("âœ… ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
        return "Healthy"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†: {e}")
        raise

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡
get_config_task = PythonOperator(
    task_id='get_table_config',
    python_callable=get_table_config,
    dag=kafka_to_ch_dag,
)

check_connections_task = PythonOperator(
    task_id='check_connections',
    python_callable=check_connections,
    dag=kafka_to_ch_dag,
)

generate_sql_task = PythonOperator(
    task_id='generate_sql_script',
    python_callable=generate_sql_script,
    dag=kafka_to_ch_dag,
)

execute_sql_task = PythonOperator(
    task_id='execute_sql_script',
    python_callable=execute_sql_script,
    dag=kafka_to_ch_dag,
)

verify_flow_task = PythonOperator(
    task_id='verify_data_flow',
    python_callable=verify_data_flow,
    dag=kafka_to_ch_dag,
)

health_check_task = PythonOperator(
    task_id='health_check',
    python_callable=health_check,
    dag=kafka_to_ch_dag,
)

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
get_config_task >> check_connections_task >> generate_sql_task >> execute_sql_task >> verify_flow_task >> health_check_task
